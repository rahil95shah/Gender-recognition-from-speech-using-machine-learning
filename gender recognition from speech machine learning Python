#===============================================================================#
#
# Only use if you use Google CoLab
# # Use to import files
#
#  from google.colab import files
#  uploaded = files.upload()
#
#===============================================================================#
# Import libraries
import statistics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn import svm
from sklearn import preprocessing
from sklearn import manifold, datasets
from sklearn.manifold import SpectralEmbedding
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.utils import shuffle

# To choose DR Method, Uncomment the selection
# Here parameters are only optimized for LTSA method with KNN classifier

#DR = 'LE'
#DR = 'HLLE'
DR = 'LTSA'
#DR = None

# To choose Classification Method, Uncomment the selection

Classification_method = 'KNN'
#Classification_method = 'DT'
#Classification_method = 'SVM'
#Classification_method = None

# To choose the name of Output file
method = str(DR) + str('_') + str(Classification_method) + str('_')

name_of_file = 'voice.csv'

# df = pd.read_excel (name_of_file) # If file is in xlsx format
df = pd.read_csv(name_of_file)

# To remove column with high co-relation (95% Here)
corr_matrix = df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
cols_to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]

k = df.drop(cols_to_drop, axis=1)
df = k
k = 1

# To shuffle the data
df_1 = shuffle(df)
df = df_1

# separate data and class
y = df.iloc[:, -1]
x = df.drop(df.columns[-1], axis=1)

k_temp = x

# Min-Max Scaling of the data (Scale the values between 0 & 1)
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
x = x_scaled

# To print the size of X and Y (class)
print(x.shape)
print(y.shape)

main_accuracy = []

q_component = 0
# Loop to calculate optimised number of dimension using q_component
# Once the optimum dimension is found, Use N as a starting and N+1 as the ending value in for loop

for n_component in range(16, 17):

    q_component += 1
    print('Num of Component :: ', n_component)

    # LE - Locally Linear Embedding

    if (DR == 'LE'):
        clf = SpectralEmbedding(n_components=n_component, affinity='rbf', n_neighbors=300)
        X_le = clf.fit_transform(k_temp)
        x = X_le
        # print(x.shape)

    # HLLE - Hessian-based Locally Linear Embedding

    if (DR == 'HLLE'):
        hlle = manifold.LocallyLinearEmbedding(n_neighbors=300, n_components=n_component, eigen_solver='dense', method='hessian')
        X_hlle = hlle.fit_transform(k_temp)
        x = X_hlle
        # print(x.shape)

    # LTSA - Local Tangent Space Alignment

    if (DR == 'LTSA'):
        clf = manifold.LocallyLinearEmbedding(n_neighbors=300, n_components=n_component, eigen_solver='dense', method='ltsa')
        X_ltsa = clf.fit_transform(k_temp)
        x = X_ltsa
        # print(x.shape)

    # Train - Test Split
    # Here data is split in 80% training and 20% testing set
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)

    # KNN - K NEAREST NEIGHBOUR

    if (Classification_method == 'KNN'):
        clf = KNeighborsClassifier(n_neighbors=3, algorithm='auto', leaf_size=3)
        nbc = clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)

    # Decision Tree

    if (Classification_method == 'DT'):
        clf = DecisionTreeClassifier(criterion='entropy', random_state=1000)
        dt = clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)  # X_validation)

    # SVM - Support Vector Machine

    if (Classification_method == 'SVM'):
        clf = svm.SVC(kernel='rbf', decision_function_shape='ovr', gamma='scale', C=100)
        svm = clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)  # X_validation)

    # To us K-fold Cross-validation
    # Here K=10
    num_validations = 10

    accuracy = cross_val_score(clf, X_train, y_train, scoring = 'accuracy', cv = num_validations)
    main_accuracy.append(statistics.mean(accuracy))

# Print the overall accuracy
print(main_accuracy)

# To prepare Confusion Matrix
conf_mat_print = metrics.confusion_matrix(y_test, y_pred)

# TO draw Confusion Matrix
sns.heatmap(conf_mat_print, annot=True, cmap='jet')

# Setting rest of the parameters to plot the Confusion matrix
name_out_graph = 'Final_Res' + '.jpg'
plt.rcParams.update({'font.size': 3})
f = plt.figure(1)
plt.savefig(name_out_graph, dpi=1000, fontsize=3, bbox_inches='tight')
plt.clf()
